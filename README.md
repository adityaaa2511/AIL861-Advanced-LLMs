# AIL861-Advanced-LLMs
### Mini Transformer Language Model â€” Decoder-Only GPT-Style Model (TinyStories)

This repository contains a complete **decoder-only Transformer language model implemented from scratch in PyTorch**, as part of the **AIL861: Advanced LLMs (IIT Delhi, Fall â€™25)** coursework.  
The project demonstrates how to build, train, and sample from a GPT-style autoregressive language model on a small dataset.

---

## ğŸš€ Key Technical Features

### âœ”ï¸ **Custom Tokenization & Vocabulary**
- Regex-based word tokenizer using `re.findall`
- Vocabulary of **~30K tokens** built from TinyStories
- Special tokens: `<pad>`, `<unk>`, `<sos>`, `<eos>`
- Manual `encode()` converts text to token IDs with length control

---

### ğŸ“š **Dataset: TinyStories (HuggingFace)**
- Loaded via `load_dataset("roneneldan/TinyStories")`
- Teacher forcing used during training:
  - Input = sequence[:-1]
  - Target = sequence[1:]
- Fixed context length (e.g., **64 tokens**)

---

## ğŸ§© Transformer Architecture

### ğŸ—ï¸ **Decoder-Only Transformer (GPT-style)**
- **3 Transformer decoder blocks**
- **300-dimensional embeddings**
- **6 attention heads**
- **FFN dimension = 4Ã— model dimension (1200)**
- **LayerNorm + Residual connections everywhere**

### ğŸ’¡ **Causal Masking**
- Ensures autoregressive constraint:
  > token i can only attend to positions â‰¤ i

---

### ğŸ§  Multi-Head Self Attention (MHA)
- Manual implementation of:
  - Q, K, V projections
  - head splitting (B, H, L, d)
  - `scaled_dot_product_attention`
- Softmax over attention scores
- Output projection + dropout

---

### ğŸ”¥ Feed-Forward Network (FFN)
- Position-wise MLP per token: Linear â†’ GELU â†’ Dropout â†’ Linear
- Provides **non-linearity + feature expansion**
- Essential for model expressivity

---

### ğŸï¸ KV-Cache for Fast Generation
- Stores past keys/values across decoding steps
- Concatenates only new tokens
- Avoids quadratic re-computation
- Enables efficient auto-regressive loops

---

## ğŸª„ Positional Embeddings
- **Sinusoidal positional embeddings** from Vaswani et al.
- Supports `start_pos` offset for cached decoding

---

## ğŸ“ Training Setup

### âš™ï¸ Hyperparameters
- Batch size: **16**
- LR: **3e-4**
- Epochs: **3â€“10**
- Optimizer: **Adam**

### ğŸš€ Mixed Precision + Gradient Accumulation
- Uses `torch.amp.autocast("cuda")`
- Uses `torch.amp.GradScaler` to avoid underflow
- Accumulation allows **simulated larger batch sizes**

---

### ğŸ“‰ Metrics
- **Cross-entropy loss**
- **Perplexity = exp(loss)**
- Logged for both train/validation
- Saved plots:
  - `loss.png`
  - `perplexity.png`

---

## ğŸ—£ï¸ Text Generation

### ğŸ² Sampling Options
- Temperature scaling
- Top-k filtering
- Multinomial sampling

### ğŸ”­ Beam Search
- Configurable beam size
- Returns sequence with highest log-probability

### ğŸ§‘â€ğŸ’» Output
Prints:
- Prompt ID sequence â†’ converted back to tokens
- Final generated text

---

## ğŸ› ï¸ Saving / Loading the Model
- Full model saved via:
```python
torch.save(model.state_dict(), "decoder_tinystories.pt")

### Sample Prompt + Generation
- Prompt: <sos> spot spot saw the
- Generated: <sos> spot spot saw the sun rise in the sky he wanted to go outside and play he ran to his mom and said mom can i go outside and play his mom smiled and said yes spot you can go outside but be careful don t go too far and don t get too close to the sun spot ran outside and saw <eos>
